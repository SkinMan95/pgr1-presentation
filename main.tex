\documentclass[10pt]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage[english,spanish]{babel}
\usepackage{relsize}
% \usepackage[utf8]{inputenc} % Required for inputting international characters
% \usepackage[T1]{fontenc} % Output font encoding for international characters

\usepackage{booktabs}
\usepackage{bm}
\usefonttheme{professionalfonts}
\usepackage{mathspec}
\usepackage{todonotes}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\title{Advanced Natural Language Processing Techniques to Profile Cybercriminals}
\subtitle{\textsc{Proyecto de grado}}
\date{\today}
% \date{}
\institute{Escuela Colombiana de Ingenier\'{\i}a Julio Garavito}
\titlegraphic{\hfill\includegraphics[height=1.5cm]{Images/escuela-logo.png}}

% 1.Nombre del proyecto
% 1.Integrantes
% 1.Director
% 2.Agenda
% 3.Objetivo general y específicos (para todo el proyecto: PGR1 y PGR2)
% 4.Justificación del proyecto
% 5.Resultados propuestos
% 5.Productos obtenidos a la fecha (explicación técnica de los resultados) (Seccion 3)
% 6.Marco teorico (x3)
% 7.Problemas y soluciones
% 7.Modelo 1 (x3)
% 7.Modelo 2 (x3)
% 8.Trabajo futuro y conclusiones

\begin{document}

\author{%
  \begin{tabular}{rl}
    \textsc{\scriptsize \textbf{Autor}}    & Alejandro \textsc{Anzola \'Avila} \\
    \textsc{\scriptsize \textbf{Director}} & Daniel Orlando \textsc{D\'{\i}az L\'opez}, \textit{PhD} \\
  \end{tabular}
  \vspace{1em}
}

\input{math_commands.tex} % para definir una notacion

\maketitle

\begin{frame}{Agenda}
  \setbeamertemplate{section in toc}[sections numbered]
  % \tableofcontents[hideallsubsections]
  \tableofcontents
\end{frame}

\section{Objetivos y justificación}

\begin{frame}{Objetivo general}
  Generar herramientas y estrategias para el perfilado de cibercriminales con ayuda de metodologías de \emph{NLP} aplicado a datos recolectados de comunicaciones y redes sociales.
\end{frame}

\begin{frame}{Objetivos específicos}
  \begin{itemize}
  \item Diseñar e implementar una solución de lenguaje natural para realizar el perfilado de sospechosos.

  \item Identificar el estado del arte en sistemas que usan \emph{NLP} para apoyar agencias de seguridad del Estado.
    
  \item Implementación de artefactos para la construcción de \emph{datasets} con información recolectada de medios privados como de fuentes abiertas.
    
  \item Validar la solución desarrollada frente a un escenario real.

  \item Modelado de diferentes metodologías, heurísticas y meta--heurísticas para \emph{NLP}.
  \end{itemize}
  
\end{frame}

\begin{frame}{Justificación}
  \todo[inline]{Por hacer}
\end{frame}

\section{Resultados propuestos y productos obtenidos}

\begin{frame}{Resultados propuestos}
  \todo[inline]{Por hacer}
\end{frame}

\begin{frame}[allowframebreaks]{Productos obtenidos}
  \begin{enumerate}
  \item Entendimiento de las generalidades de \textsc{Data Science}:
    \begin{itemize}
    \item Tipos de \textsc{Machine Learning}
    \item Sistemas de detección de anomalías
    \item Diferentes modalidades de clustering
    \end{itemize}
  \item Identificacion de modelos de \textsc{NLP} aplicables para el perfilado de cibercriminales
    
  \item Entendimiento de los modelos de clasificación y clustering:
    \begin{itemize}
    \item Clasificador de Na\"ive Bayes
    \item Maquinas de soporte vectorial
    \item Mapas autoorganizados
    \end{itemize}
    
  \item Entendimiento de los modelos utilizados en \textsc{NLP}:
    \begin{itemize}
    \item Predicción de etiquetas con modelos de regresión lineal
    \item Reconocimiento de \textsc{Named Entities}
    \item Uso de \emph{embeddings} generados con \textsc{StarSpace} para los $k$ textos mas similares
    \end{itemize}
    
  \item Implementación de modelos de \textsc{NLP} para el perfilado de cibercriminales:
    \begin{itemize}
    \item Modelo de predicción de hashtags de Twitter con modelos lineales
    \item Modelo de reconocimiento de \textsc{Named Entities} con redes \textsc{LSTM}
    \item Modelo de clustering en redes \textsc{SOM} con \emph{embeddings} de \textsc{StarSpace}
    \end{itemize}
    
  \end{enumerate}
\end{frame}

\section{Marco teórico}

\subsection{Clasificador \textsc{Na\"ive Bayes}}
\begin{frame}{Teorema de Bayes}
  % \todo[inline]{Por hacer}
  Para variables aleatorias $\rx$ e $\ry$, se tiene que la probabilidad condicional $P(\ry \mid \rx)$ es definida como
  \begin{equation*}
    P(\ry \mid \rx) = \frac{P(\rx \mid \ry) P(\ry)}{P(\rx)}
  \end{equation*}
\end{frame}

\begin{frame}{Clasificador \textsc{Na\"ive Bayes}}
  Un clasificador de Na\"{\i}ve Bayes estima la probabilidad condicional de las clases por medio de suponer que los atributos son condicionalmente independientes, dado la etiqueta de clasificación $y$. Donde cada conjunto de $d$ atributos $\sX=\{ x_1, \ldots, x_d \}$ se tiene
  \begin{equation*}
    P(\sX\mid\ry=y) = \prod_{i=1}^{d} P(x_i\mid\ry=y)
  \end{equation*}

  El clasificador computa la probabilidad posterior para cada clase $\ry$ como
  \begin{equation*}
    P(\ry\mid \sX) = \frac{P(\ry) \prod_{i=1}^{d}P(x_i\mid\ry)}{P(\sX)} \Rightarrow P(\ry) \prod_{i=1}^{d}P(x_i\mid\ry)
  \end{equation*}
  \alert{Nota} Puede ignorarse $P(\sX)$ debido a que es un termino constante. Para esto se realiza una normalización con una constante $\epsilon$ de forma que $\sum_{\forall \ry \in \sY} \epsilon^{-1} P(\ry\mid \sX) = 1$.
\end{frame}

\subsection{Clasificación con \textsc{Support Vector Machines (SVM)}}
\begin{frame}{Clasificación con \textsc{Support Vector Machines (SVM)}}
  Técnica de clasificación con una frontera de decisión en forma de hiper-planos que permiten aplicaciones con vectores de alta dimensionalidad. En su forma no--lineal se utiliza una función polinomial de similaridad $K$ para no afectar el rendimiento por la alta dimensionalidad.
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Images/svm-hyperplanes.pdf}
    \caption[Maximum Margin Hyperplanes]{Maximum Margin Hyperplanes. Tomado de \cite{tan2005introduction}.}
    \label{fig:svm-hyperplanes}
  \end{figure}
\end{frame}

\subsection{\textsc{Self-organizing Maps (SOM)}}
\begin{frame}{\textsc{Self-organizing Maps (SOM)}}
  Es un mapa discreto de $o$ neuronas con vectores $\vw \in \R^m$ que se adaptan a una entrada de $\mX \in \R^{m \times N}$ de $N$ patrones. Tiene una adaptación con una tasa de aprendizaje $\alpha_t$ y un área de afectación $\sigma_t$ que se reducen por cada iteración $t \in \{0, \ldots, T\}$.
  \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{Images/som-adaptive-proc.pdf}
    \caption[Proceso de adaptación de \textsc{SOM}]{Proceso de adaptación de \textsc{SOM}, (a)~uni--dimensional, (b)~bi--dimensional. Tomado de \cite{de2006fundamentals}.}
    \label{fig:som-adap-proc}
  \end{figure}
\end{frame}

\begin{frame}{Ejemplo de \textsc{SOM}}
  \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{Images/som-implementation-example25.pdf}
    \caption[Ejemplo de salida de \textsc{som} uni-dimensional]{Ejemplo de salida de \textsc{som} uni-dimensional con 25 neuronas. Implementación propia.}
    \label{fig:som-impl-example}
  \end{figure}
\end{frame}

\begin{frame}{Aplicación de \textsc{SOM} en perfilamiento de criminales}
  \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{Images/som-example.png}
    \caption[Ejemplo de uso de \textsc{SOM} en aplicaciones de perfilado]{Ejemplo de uso de \textsc{SOM} en aplicaciones de perfilado. Tomado de \cite{mena2003investigative}.}
    \label{fig:som-example}
  \end{figure}
\end{frame}

% ================================================================

\section{Problemas y soluciones}

\subsection{\textsc{Modelo 1:} Predicción de etiquetas de Twitter}

\begin{frame}{Representación de palabras: \textsc{Bag of Words}}
  $N$ es el tamaño del diccionario de términos $D$ (e.g. $N = |D|$).
  \begin{equation*} \label{eq:bow-repr1}
    \text{word2idx} = \Big\{(t_i, i) : \forall i \in \{1, \ldots, N\} \Big\}
  \end{equation*}
  
  \begin{equation*} \label{eq:bow-repr2}
    \text{idx2word} = \Big[t_1, \ldots, t_N\Big]
  \end{equation*}

  \begin{alertblock}{Representación de palabras en vectores para \textsc{BoW}}
    Para un termino individual su vector representativo se define como:
    \begin{equation*}
      \ve^{(i)}, (t, i) \in \text{word2idx}
    \end{equation*}
    Para un documento $d$ de términos, se calcula por cada termino que existen dentro del diccionario su vector representativo como:
    \begin{equation*} \label{eq:bow-word-vector-sum}
      \vs = \mathlarger{\mathlarger{\sum}}_{(t, i) \in \text{word2idx}} \ve^{(i)}, t \in d
    \end{equation*}
  \end{alertblock}
\end{frame}

\begin{frame}{Representación de palabras: \textsc{TF--IDF}}
  \textsc{TF--IDF} $=$ Term Frequency -- Inverse Document Frequency
  \begin{equation*} \label{eq:tf-repr}
    \text{tf}(t,d) = \text{Frecuencia del termino (o n--grama) } t \text{ en el documento } d
  \end{equation*}

  \begin{equation*} \label{eq:idf-repr}
    \text{idf}(t, D) = \text{log}\Bigg( \frac{N}{|\{d \in D : t \in d\}|} \Bigg) ; N = |D|
  \end{equation*}

  \begin{equation*} \label{eq:tfidf-repr}
    \text{tf-idf}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D)
  \end{equation*}
\end{frame}

\begin{frame}{Regresión logística $\sigma(x)$ y One vs Rest}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \begin{axis}[ 
        xlabel=$x$,
        ylabel={$\sigmoid(x)$}
        ] 
        \addplot {1 / (1+exp(-x))}; 
      \end{axis}
    \end{tikzpicture}
    \caption[Gráfica de función sigmoide]{Gráfica de función sigmoide.}
    \label{fig:logits-example}
  \end{figure}
\end{frame}

\begin{frame}{\textsc{Modelo 1:} Predicción de etiquetas de Twitter}
  \todo[inline]{Por hacer}
\end{frame}

\subsection{\textsc{Modelo 2:} Reconocimiento de \textsc{Named Entities} con redes \textsc{LSTM}}

\begin{frame}{\textsc{Modelo 2:} Reconocimiento de \textsc{Named Entities} con redes \textsc{LSTM}}
  \todo[inline]{Por hacer}
\end{frame}

\section{Conclusiones y trabajo futuro}

\begin{frame}{Conclusiones}
  \todo[inline]{Por hacer}
\end{frame}

\begin{frame}{Trabajo futuro}
  \todo[inline]{Por hacer}
\end{frame}

\appendix
\setbeamertemplate{bibliography item}{\insertbiblabel}
\begin{frame}[allowframebreaks]{Bibliografía}

  % \nocite{*}
  \bibliography{demo}
  \bibliographystyle{abbrv}

\end{frame}

\end{document}
