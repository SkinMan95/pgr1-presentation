\documentclass[10pt]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage[english,spanish]{babel}
\usepackage{relsize}
% \usepackage[utf8]{inputenc} % Required for inputting international characters
% \usepackage[T1]{fontenc} % Output font encoding for international characters

\usepackage{booktabs}
\usepackage{bm}
\usefonttheme{professionalfonts}
\usepackage{mathspec}
\usepackage{todonotes}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\title{Advanced Natural Language Processing Techniques to Profile Cybercriminals}
\subtitle{\textsc{Proyecto de grado 1}}
\date{\today}
% \date{}
\institute{Escuela Colombiana de Ingenier\'{\i}a Julio Garavito}
\titlegraphic{\hfill\includegraphics[height=1.5cm]{Images/escuela-logo.png}}

% 1.Nombre del proyecto
% 1.Integrantes
% 1.Director
% 2.Agenda
% 3.Objetivo general y específicos (para todo el proyecto: PGR1 y PGR2)
% 4.Justificación del proyecto
% 5.Resultados propuestos
% 5.Productos obtenidos a la fecha (explicación técnica de los resultados) (Seccion 3)
% 6.Marco teorico (x3)
% 7.Problemas y soluciones
% 7.Modelo 1 (x3)
% 7.Modelo 2 (x3)
% 8.Trabajo futuro y conclusiones

\begin{document}

\author{%
  \begin{tabular}{rl}
    \textsc{\scriptsize \textbf{Autor}}    & Alejandro \textsc{Anzola \'Avila} \\
    \textsc{\scriptsize \textbf{Director}} & Daniel Orlando \textsc{D\'{\i}az L\'opez}, \textit{PhD} \\
  \end{tabular}
  \vspace{1em}
}

\input{math_commands.tex} % para definir una notacion

\maketitle

\begin{frame}{Agenda}
  \setbeamertemplate{section in toc}[sections numbered]
  % \tableofcontents[hideallsubsections]
  \tableofcontents
\end{frame}

\section{Objetivos y justificación}

\begin{frame}{Objetivo general}
  Generar herramientas y estrategias para el perfilado de cibercriminales con ayuda de metodologías de \emph{NLP} aplicado a datos recolectados de comunicaciones y redes sociales.
\end{frame}

\begin{frame}{Objetivos específicos}
  \begin{itemize}
  \item Diseñar e implementar una solución de lenguaje natural para realizar el perfilado de sospechosos.

  \item Identificar el estado del arte en sistemas que usan \emph{NLP} para apoyar agencias de seguridad del Estado.
    
  \item Implementación de artefactos para la construcción de \emph{datasets} con información recolectada de medios privados como de fuentes abiertas.
    
  \item Validar la solución desarrollada frente a un escenario real.

  \item Modelado de diferentes metodologías, heurísticas y meta--heurísticas para \emph{NLP}.
  \end{itemize}
  
\end{frame}

\begin{frame}{Justificación}
  En EE.UU. antes del 11 de septiembre, las agencias de seguridad ponían mas énfasis en \textbf{reaccionar} ante los eventos en vez de \textbf{prevenirlos} \cite{mena2003investigative}.

  El trabajo del actual proyecto se enfoca en heurísticas y meta-heurísticas para prevenir incidentes al identificar a posibles perpetradores.
\end{frame}

\section{Resultados propuestos y productos obtenidos}

\begin{frame}{Resultados propuestos}
  \begin{enumerate}
  \item Entendimiento del uso de \textsc{Data Science} en ciber-inteligencia
  \item Entendimiento de las heurísticas de \textsc{NLP}
  \item Proponer modelos de \textsc{Machine Learning} para la identificación de cibercriminales
  \item Implementación de los modelos propuestos con sistemas Estado-del-arte
  \item Pruebas de eficacia y eficiencia de los modelos propuestos para facilitar la tarea de perfilado
  \end{enumerate}
\end{frame}

\begin{frame}[allowframebreaks]{Productos obtenidos}
  \begin{enumerate}
  \item Entendimiento de las generalidades de \textsc{Data Science}:
    \begin{itemize}
    \item Tipos de \textsc{Machine Learning}
    \item Sistemas de detección de anomalías
    \item Diferentes modalidades de clustering
    \end{itemize}
  \item Identificacion de modelos de \textsc{NLP} aplicables para el perfilado de cibercriminales
    
  \item Entendimiento de los modelos de clasificación y clustering:
    \begin{itemize}
    \item Clasificador de Na\"ive Bayes
    \item Maquinas de soporte vectorial
    \item Mapas autoorganizados
    \end{itemize}
    
  \item Entendimiento de los modelos utilizados en \textsc{NLP}:
    \begin{itemize}
    \item Predicción de etiquetas con modelos de regresión lineal
    \item Reconocimiento de \textsc{Named Entities}
    \item Uso de \emph{embeddings} generados con \textsc{StarSpace} para los $k$ textos mas similares
    \end{itemize}
    
  \item Propuesta de modelos de \textsc{NLP} para el perfilado de cibercriminales:
    \begin{itemize}
    \item Modelo de predicción de hashtags de Twitter con modelos lineales
    \item Modelo de reconocimiento de \textsc{Named Entities} con redes \textsc{LSTM}
    \item Búsqueda de tweets relacionados con \emph{embeddings} de \textsc{StarSpace}
    \item Modelo de clustering en redes \textsc{SOM} con \emph{embeddings} de \textsc{StarSpace}
    \end{itemize}
    
  \end{enumerate}
\end{frame}

\section{Marco teórico}

\subsection{Clasificador \textsc{Na\"ive Bayes}}
\begin{frame}{Teorema de Bayes}
  % \todo[inline]{Por hacer}
  Para variables aleatorias $\rx$ e $\ry$, se tiene que la probabilidad condicional $P(\ry \mid \rx)$ es definida como
  \begin{equation*}
    P(\ry \mid \rx) = \frac{P(\rx \mid \ry) P(\ry)}{P(\rx)}
  \end{equation*}
\end{frame}

\begin{frame}{Clasificador \textsc{Na\"ive Bayes}}
  Un clasificador de Na\"{\i}ve Bayes estima la probabilidad condicional de las clases por medio de suponer que los atributos son condicionalmente independientes, dado la etiqueta de clasificación $y$. Donde cada conjunto de $d$ atributos $\sX=\{ x_1, \ldots, x_d \}$ se tiene
  \begin{equation*}
    P(\sX\mid\ry=y) = \prod_{i=1}^{d} P(x_i\mid\ry=y)
  \end{equation*}

  El clasificador computa la probabilidad posterior para cada clase $\ry$ como
  \begin{equation*}
    P(\ry\mid \sX) = \frac{P(\ry) \prod_{i=1}^{d}P(x_i\mid\ry)}{P(\sX)} \Rightarrow P(\ry) \prod_{i=1}^{d}P(x_i\mid\ry)
  \end{equation*}
  \alert{Nota} Puede ignorarse $P(\sX)$ debido a que es un termino constante. Para esto se realiza una normalización con una constante $\epsilon$ de forma que $\sum_{\forall \ry \in \sY} \epsilon^{-1} P(\ry\mid \sX) = 1$.
\end{frame}

\subsection{Clasificación con \textsc{Support Vector Machines (SVM)}}
\begin{frame}{Clasificación con \textsc{Support Vector Machines (SVM)}}
  Técnica de \textbf{clasificación} con una frontera de decisión en forma de hiper-planos que permiten aplicaciones con vectores de alta dimensionalidad.
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Images/svm-hyperplanes.pdf}
    \caption[Maximum Margin Hyperplanes]{Maximum Margin Hyperplanes. Tomado de \cite{tan2005introduction}.}
    \label{fig:svm-hyperplanes}
  \end{figure}
\end{frame}

\subsection{\textsc{Self-organizing Maps (SOM)}}
\begin{frame}{\textsc{Self-organizing Maps (SOM)}}
  Es un mapa discreto de $o$ neuronas con vectores $\vw \in \R^m$ que se adaptan a una entrada de $\mX \in \R^{m \times N}$ de $N$ patrones. Tiene una adaptación con una tasa de aprendizaje $\alpha_t$ y un área de afectación $\sigma_t$ que se reducen por cada iteración $t \in \{0, \ldots, T\}$.
  \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{Images/som-adaptive-proc.pdf}
    \caption[Proceso de adaptación de \textsc{SOM}]{Proceso de adaptación de \textsc{SOM}, (a)~uni--dimensional, (b)~bi--dimensional. Tomado de \cite{de2006fundamentals}.}
    \label{fig:som-adap-proc}
  \end{figure}
\end{frame}

% \begin{frame}{Ejemplo de \textsc{SOM}}
%   \begin{figure}
%     \centering
%     \includegraphics[width=0.75\textwidth]{Images/som-implementation-example25.pdf}
%     \caption[Ejemplo de salida de \textsc{som} uni-dimensional]{Ejemplo de salida de \textsc{som} uni-dimensional con 25 neuronas. Implementación propia.}
%     \label{fig:som-impl-example}
%   \end{figure}
% \end{frame}

% \begin{frame}{Aplicación de \textsc{SOM} en perfilamiento de criminales}
%   \begin{figure}
%     \centering
%     \includegraphics[width=0.75\textwidth]{Images/som-example.png}
%     \caption[Ejemplo de uso de \textsc{SOM} en aplicaciones de perfilado]{Ejemplo de uso de \textsc{SOM} en aplicaciones de perfilado. Tomado de \cite{mena2003investigative}.}
%     \label{fig:som-example}
%   \end{figure}
% \end{frame}

% ================================================================

\section{Problemas y soluciones}

\subsection{\textsc{Modelo 1:} Predicción de etiquetas de Twitter}

\begin{frame}{\textsc{Modelo 1:} Problema}
  \begin{alertblock}{Tweet}
    \begin{quote}
    ``Really excited to add @plaidavenger to my \textbf{\#deathlist} along with Italy and @Plaid\_Obama after receiving that information. \textbf{\#KillEveryone} \textbf{\#ISIS}''
    \end{quote}
  \end{alertblock}
\end{frame}

\begin{frame}{\textsc{Modelo 1:} Predicción de etiquetas de Twitter}
  \begin{alertblock}{?`Que hacer?}
    Con un modelo de regresión lineal predecir los hashtags de los tweets.
  \end{alertblock}
  \vspace{2em}
  
  \begin{tabular}{p{0.47\textwidth} p{0.05\textwidth} p{0.46\textwidth}}
    ``Really excited to add @plaidavenger to my deathlist along with Italy and @Plaid\_Obama after receiving that information.'' & $\mathlarger{\mathlarger{\mathlarger{\Rightarrow}}}$ & \textbf{\#deathlist, \#KillEveryone, \#ISIS}
    \end{tabular}
\end{frame}

\begin{frame}{Representación de palabras: \textsc{Bag of Words}}
  $N$ es el tamaño del diccionario de términos $D$ (e.g. $N = |D|$).
  \begin{equation*} \label{eq:bow-repr1}
    \text{word2idx} = \Big\{(t_i, i) : \forall i \in \{1, \ldots, N\} \Big\}
  \end{equation*}
  
  \begin{equation*} \label{eq:bow-repr2}
    \text{idx2word} = \Big[t_1, \ldots, t_N\Big]
  \end{equation*}

  \begin{alertblock}{Representación de palabras en vectores para \textsc{BoW}}
    Para un termino individual su vector representativo se define como:
    \begin{equation*}
      \ve^{(i)} = [0, \ldots, 1, \ldots, 0] \leftarrow \text{posicion } i\text{--\'esima}
    \end{equation*}
    \begin{equation*}
      \ve^{(i)}, (t, i) \in \text{word2idx}
    \end{equation*}
    Para un documento $d$ de términos, se calcula por cada termino que existen dentro del diccionario su vector representativo como:
    \begin{equation*} \label{eq:bow-word-vector-sum}
      \vs = \mathlarger{\mathlarger{\sum}}_{(t, i) \in \text{word2idx}} \ve^{(i)}, t \in d
    \end{equation*}
  \end{alertblock}
\end{frame}

\begin{frame}{Representación de palabras: \textsc{TF--IDF}}
  \textsc{TF--IDF} $=$ Term Frequency -- Inverse Document Frequency
  \begin{alertblock}{Propósito}
    Darle mayor importancia a las palabras que ocurren con frecuencia intermedia en el documento $d$ y en el corpus $D$.
  \end{alertblock}
  \begin{equation*} \label{eq:tf-repr}
    \text{tf}(t,d) = \text{Frecuencia del termino (o n--grama) } t \text{ en el documento } d
  \end{equation*}

  \begin{equation*} \label{eq:idf-repr}
    \text{idf}(t, D) = \text{log}\Bigg( \frac{N}{|\{d \in D : t \in d\}|} \Bigg) ; N = |D|
  \end{equation*}

  \begin{equation*} \label{eq:tfidf-repr}
    \text{tf-idf}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D)
  \end{equation*}
\end{frame}

\begin{frame}{Regresión lineal}
  Para una vector de parámetros $\vtheta$ y un vector de características $\vx$, la regresión lineal se puede definir como:
  \begin{equation*}
    \hat{y}(\vx, \vtheta) = \vtheta^{\top} \vx = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n
  \end{equation*}

  Donde $\hat{y}(\vx, \vtheta) \,:\, \R^n \times \R^n \rightarrow \R$.
  
  $\theta_0$ se le conoce como el \emph{bias} del modelo.

  El objetivo es que para una salida esperada $y$ se tenga la salida $\hat{y}$ con menor error por medio de ajustar los valores de $\vtheta$. De forma que se quiere:
  \begin{equation*}
    \vtheta = \text{arg m\'{\i}n}_{\vtheta} | \hat{y}(\vx, \vtheta) - y|
  \end{equation*}

\end{frame}

\begin{frame}{Regresión logística $\sigma(x)$}
  \noindent\begin{minipage}{0.6\textwidth}
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}[scale=0.80]
        \begin{axis}[ 
          xlabel=$x$,
          ylabel={$\sigmoid(x)$}
          ] 
          \addplot {1 / (1+exp(-x))}; 
        \end{axis}
      \end{tikzpicture}
      \caption[Gráfica de función sigmoide]{Gráfica de función sigmoide.}
      \label{fig:logits-example}
    \end{figure}
  \end{minipage}%
  \hfill%
  \begin{minipage}{0.35\textwidth}
    \begin{equation*}
      \sigmoid(x) = \frac{1}{1+\exp(-x)}
    \end{equation*}
    
    \begin{equation*}
      \sigmoid(x) \,:\, \R \rightarrow (0,1)
    \end{equation*}
    Evita problemas de \textsc{Bias} y \textsc{Overfitting} del modelo
  \end{minipage}
\end{frame}

\begin{frame}{One vs Rest}
  \noindent\begin{minipage}{0.5\textwidth}
    \begin{figure}[H]
      \centering
      % \missingfigure{Hacer la arquitectura en yEd}
      \includegraphics[width=\textwidth]{Images/one-vs-rest.pdf}
      \caption[Algoritmo de One vs Rest]{Algoritmo de One vs Rest.}
      \label{fig:ovr-algo}
    \end{figure}
  \end{minipage}%
  \hfill%
  \begin{minipage}{0.45\textwidth}
    Se entrenan $C$ estimadores $\vtheta_i$ para cada clase con algún algoritmo de optimización (ej. gradiente descendiente). \\
    
    Se determina un estimador $c \in \{1, \ldots ,C\}$, que se calcula como:
    \begin{equation*}
      c = \text{arg max}_i \, \sigmoid(\vtheta_i^{\top} \vx)
    \end{equation*}
  \end{minipage}
\end{frame}

\begin{frame}{Predicción de hashtags}
  A partir de un diccionario previamente definido a entrenar el \textsc{One vs Rest} de forma:
  \begin{equation*}
    \{(i, h)\} \,;\, h \in \text{hashtags} \,;\, i \in \{1, \ldots, C\}
  \end{equation*}

  De forma que se recupera el hashtag $h$ correspondiente a partir de la clase estimada $i$ por \textsc{One vs Rest}.
\end{frame}

\subsection{\textsc{Modelo 2:} Reconocimiento de \textsc{Named Entities} con redes \textsc{LSTM}}

\begin{frame}{\textsc{Modelo 2:} Problema}
  \alert{?`De que y de quienes están hablando?}
  
\begin{alertblock}{Tweet: @realDonaldTrump}
  \begin{quote}
    ``The \textbf{Democrats} new and pathetically untrue sound bite is that we are in a “Constitutional Crisis.” They and their partner, the \textbf{Fake News Media}, are all told to say this as loud and as often as possible. They are a sad JOKE! We may have the strongest \textbf{Economy} in our history, best ...''
  \end{quote}
\end{alertblock}
\end{frame}

\begin{frame}{\textsc{Modelo 2:} Reconocimiento de \textsc{Named Entities} con redes \textsc{LSTM}}
  Son redes neuronales recurrentes que son capaces de reconocer \textsc{Named Entities}.
  \begin{table}
    \centering
    \begin{tabular}{l|lllllll} 
      \textbf{Texto}    & Donald   & Trump & es & presidente & de & Estados & Unidos \\
      \textbf{Etiqueta} & B-PER    & I-PER & O  & O          & O  & B-ORG   & I-ORG
    \end{tabular}
    \\ [1em]
    \caption{Ejemplo de reconocimiento de \textsc{Named Entities}.}
    \label{table:namedent-example}
  \end{table}

  \begin{table}
    \centering
    \begin{tabular}{rl} 
      Otro & O \\
      Persona & PER \\
      Ubicación & LOC \\
      Organización & ORG \\
      Misceláneo & MISC \\
    \end{tabular}
    \caption{Categorías de \textsc{Named Entities}.}
    \label{table:namedent-categories}
  \end{table}
\end{frame}

\begin{frame}{Redes neuronales recurrentes (\textsc{RNN})}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/RNN-unrolled.png}
    \caption[Red \textsc{RNN} simplificada]{Red \textsc{RNN} simplificada. Tomado de \cite{understanding-lstm}.}
    \label{fig:rnn-classic-simple}
  \end{figure}

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/LSTM3-SimpleRNN.png}
    \caption[Arquitectura \textsc{RNN} clásica]{Arquitectura \textsc{RNN} clásica. Tomado de \cite{understanding-lstm}.}
    \label{fig:rnn-classic}
  \end{figure}
\end{frame}

\begin{frame}{Redes \textsc{Long Short Term Memory (LSTM)}}
  \begin{figure}[H]
    \centering
    % \missingfigure{Hacer la arquitectura en yEd}
    \includegraphics[width=0.9\textwidth]{Images/LSTM3-chain.png}
    \caption[Arquitectura de red \textsc{LSTM} clásica]{Arquitectura de red \textsc{LSTM} clásica. Tomado de \cite{understanding-lstm}.}
    \label{fig:lstm-classic}
  \end{figure}

  \begin{alertblock}{Nota}
    Estas redes son solo \emph{feedforward} (e.g. hacia adelante). Solo se basan en entradas pasadas.
  \end{alertblock}
\end{frame}

\begin{frame}{Redes \textsc{Bidirectional Long Short Term Memory (Bi-LSTM)}}
  \begin{figure}[H]
    \centering
    % \missingfigure{Hacer la arquitectura en yEd}
    \includegraphics[width=0.6\textwidth]{Images/bilstm-arch.pdf}
    \caption[Etiquetado con una \textsc{Bi-LSTM}]{Etiquetado con una \textsc{Bi-LSTM}. Tomado de \cite{Huang2015}.}
    \label{fig:bilstm-arch}
  \end{figure}

  \begin{alertblock}{Nota}
    Estas redes son \emph{feedforward} como \emph{backward}. Se basan de entradas pasadas y futuras.
  \end{alertblock}
\end{frame}


% \begin{frame}{Entrenamiento y uso de redes \textsc{Bi-LSTM}}
%   Se procede primeramente por entrenar el modelo con un dataset, que establece la relación $(t, e)$ de las palabras $t$ y las \emph{Entities} $e$.

%   Las librerías en uso son \textsc{TensorFlow} y \textsc{Numpy}.
% \end{frame}

\subsection{\textsc{Modelo 3:} Búsqueda de tweets relacionados con \emph{embeddings}}

\begin{frame}{?`Que son los \emph{embeddings}?}
  Son espacios de vectores $n$--dimensionales que se mapean según una palabra.

  Tómese $\vp_{t}$ como el vector que representa el termino $t$ y a $d$ como la distancia calculada entre los vectores (típicamente la distancia \textbf{coseno}).

  \begin{alertblock}{Ejemplo}
    $d(\vp_{\text{asombroso}}, \vp_{\text{genial}})$ debería tener un valor bajo.
    
    $d(\vp_{\text{asombroso}}, \vp_{\text{terrible}})$ debería tener un valor alto.
  \end{alertblock}

  También se pueden representar varias palabras de un documento en un solo vector por medio de sumarlos. (i.e. $\sum_{t \in d}\vp_{t}$).
\end{frame}

\begin{frame}{\textsc{Modelo 3:} Búsqueda de tweets relacionados con \emph{embeddings}}
  Es posible categorizar los $k$ textos mas parecidos a una consulta $q$ en base a su embedding con otros textos recopilados.

  \begin{alertblock}{StarSpace}
    Genera embeddings en base a un dataset de entrenamiento. Desarrollado por Facebook Research en 2017 \cite{starspace}. 
  \end{alertblock}
\end{frame}

\section{Conclusiones y trabajo futuro}

\begin{frame}{Conclusiones}
  \begin{itemize}
  \item Se investigaron diferentes metodologías de NLP y Data Science para la tarea de perfilado de cibercriminales por medio de informacion de fuentes abiertas.
  \item Es necesario probar las metodologías propuestas con información obtenida de fuentes abiertas que este validada de forma que el entrenamiento de ellos sean efectivos en la tarea.
  \end{itemize}
\end{frame}

\begin{frame}{Trabajo futuro}
  \begin{itemize}
  \item Implementación de los modelos 2 y 3 propuestos con propósito de ayudar al perfilamiento de cibercriminales.
  \item Recopilar datos pertinentes para el entrenamiento de los modelos propuestos.
  \item Adaptar y generalizar los modelos para el uso del lenguaje español.
  \item Implementar un modelo de recolección de información de redes sociales de cibercriminales de forma que sea mas fácil perfilarlos contra futuros.
  \item Realizar una visualización en dashboard de los algoritmos propuestos para ayudar al agente a realizar el perfilamiento.
  \end{itemize}
\end{frame}

\appendix
\setbeamertemplate{bibliography item}{\insertbiblabel}
\begin{frame}[allowframebreaks]{Bibliografía}

  % \nocite{*}
  \bibliography{demo}
  \bibliographystyle{abbrv}

\end{frame}

\end{document}
